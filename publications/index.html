<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications &amp; Patents | Yongfan Liu </title> <meta name="author" content="Yongfan Liu"> <meta name="description" content="&lt;nobr&gt;*&lt;/nobr&gt; denotes equal contribution or joint lead authorship."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%81&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sf-liu.github.io//publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@700&amp;display=swap" rel="stylesheet"> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <span style="font-family: 'Dancing Script', cursive; font-weight: 700; font-size: 2em; margin-right: 8px;">Steff</span> <span style="font-size: 1.8em;">Liu</span> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications &amp; Patents <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Academic Service </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications &amp; Patents</h1> <p class="post-description"><nobr>*</nobr> denotes equal contribution or joint lead authorship.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <p><br></p> <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;"> <div class="col-sm-1 mt-2 p-0 pr-1"> <h3 class="bibliography-year">2025</h3> </div> <div class="col-sm-11 p-0"> <ol class="bibliography"> <li> <style>.pub-icon{max-width:105px;height:auto;transition:transform .3s ease;cursor:pointer}.pub-icon:hover{transform:scale(2.5);z-index:10;transform-origin:bottom right;position:relative}</style> <div class="row m-0 mt-3 p-0"> <div class="col-sm-1 p-0 abbr d-flex flex-column align-items-center "> <div class="mb-1 text-center"> <img src="/assets/img/pub/topk_avg.png" alt="arXiv icon" class="pub-icon"> </div> <span class="badge font-weight-bold light-green darken-1 text-center" style="width: 65px;"> arXiv </span> </div> <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-sm-5 pr-sm-2"> <div id="mbvggt" class="col p-0"> <h5 class="title mb-0">MBVGGT: Adapting VGGT for Long Video Sequences via Graph Memory Bank</h5> <div class="author"> <nobr><strong>Yongfan Liu<nobr><em>*</em></nobr></strong>,</nobr> <nobr>Boyuan Tian<nobr><em>*</em></nobr>,</nobr> <nobr><a href="https://www.linkedin.com/in/rahul-singh-309/" target="_blank" rel="external nofollow noopener">Rahul Singh</a>,</nobr> <nobr><a href="https://sadve.cs.illinois.edu/" target="_blank" rel="external nofollow noopener">Sarita Adve</a>,</nobr> and <nobr><a href="https://hyoukjunkwon.com/" target="_blank" rel="external nofollow noopener">Hyoukjun Kwon</a>.</nobr> </div> <div> <p class="periodical font-italic"> In Under Review of CVPR 2026 </p> </div> <div class="col p-0"> <a class="badge orange waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mbvggt-abstract" role="button" aria-expanded="false" aria-controls="mbvggt-abstract">Abstract</a> </div> <div class="col mt-2 p-0"> <div id="mbvggt-abstract" class="collapse"> <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3"> Recent advances in large-scale pretrained visual transformers have achieved remarkable success in delivering high-quality 3D reconstruction results. However, for a stream of inputs (e.g., video), they require heavy global recomputation for each new frame, which also involves quadratic memory costs to the sequence length in attention modules. To address the challenge, techniques such as caching previous keys and values have been explored. However, such approaches still incur quadratic memory costs, which hinder their deployment with long sequence inputs on memory-constrained commodity hardware. To address the challenge, we first make an observation that not all input frames are critical for processing new inputs by a quantitative analysis and utilize that observation to develop a new compute- and memory-efficient vision transformer for 3D reconstruction, MBVGGT. It employs a memory bank that maintains information of important frames only, which significantly reduces memory requirements yet enables high-quality results. Our evaluations demonstrate that MBVGGT achieves comparable accuracy across depth estimation, pose estimation, and 3D reconstruction on long content, while running 6.3x faster than the SOTA model and getting rid of the OOM error. </div> </div> </div> </div> </div> </div> </li> <li> <style>.pub-icon{max-width:105px;height:auto;transition:transform .3s ease;cursor:pointer}.pub-icon:hover{transform:scale(2.5);z-index:10;transform-origin:bottom right;position:relative}</style> <div class="row m-0 mt-3 p-0"> <div class="col-sm-1 p-0 abbr d-flex flex-column align-items-center "> <div class="mb-1 text-center"> <img src="/assets/img/pub/pos_enc.png" alt="CVPR icon" class="pub-icon"> </div> <a class="badge font-weight-bold purple darken-1 text-center" style="width: 65px;" href="https://cvpr.thecvf.com/" target="_blank" rel="external nofollow noopener"> CVPR </a> </div> <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-sm-5 pr-sm-2"> <div id="liu2025efficient" class="col p-0"> <h5 class="title mb-0">Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses</h5> <div class="author"> <nobr><strong>Yongfan Liu</strong>,</nobr> and <nobr><a href="https://hyoukjunkwon.com/" target="_blank" rel="external nofollow noopener">Hyoukjun Kwon</a>.</nobr> </div> <div> <p class="periodical font-italic"> In Proceedings of the Computer Vision and Pattern Recognition Conference 2025. </p> </div> <div class="col p-0"> <a class="badge orange waves-effect font-weight-light mr-1" data-toggle="collapse" href="#liu2025efficient-abstract" role="button" aria-expanded="false" aria-controls="liu2025efficient-abstract">Abstract</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://cvpr.thecvf.com/virtual/2025/poster/33885" target="_blank" rel="external nofollow noopener">HTML</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://sf-liu.github.io//assets/pdf/2411.10013v2.pdf" target="_blank">PDF</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://github.com/UCI-ISA-Lab/MultiHeadDepth-HomoDepth" target="_blank" rel="external nofollow noopener">Code</a> </div> <div class="col mt-2 p-0"> <div id="liu2025efficient-abstract" class="collapse"> <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3"> Stereo depth estimation is a fundamental component in augmented reality (AR), which requires low latency for real-time processing. However, preprocessing such as rectification and non-ML computations such as cost volume require significant amount of latency exceeding that of an ML model itself, which hinders the real-time processing required by AR. Therefore, we develop alternative approaches to the rectification and cost volume that consider ML acceleration (GPU and NPUs) in recent hardware. For pre-processing, we eliminate it by introducing homography matrix prediction network with a rectification positional encoding (RPE), which delivers both low latency and robustness to unrectified images. For cost volume, we replace it with a group-pointwise convolution-based operator and approximation of cosine similarity based on layernorm and dot product. Based on our approaches, we develop MultiHeadDepth (replacing cost volume) and HomoDepth (MultiHeadDepth + removing pre-processing) models. MultiHeadDepth provides 11.8-30.3% improvements in accuracy and 22.9-25.2% reduction in latency compared to a state-of-the-art depth estimation model for AR glasses from industry. HomoDepth, which can directly process unrectified images, reduces the end-to-end latency by 44.5%. We also introduce a multi-task learning method to handle misaligned stereo inputs on HomoDepth, which reduces the AbsRel error by 10.0-24.3%. The overall results demonstrate the efficacy of our approaches, which not only reduce the inference latency but also improve the model performance. </div> </div> </div> </div> </div> </div> </li> </ol> </div> </div> <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;"> <div class="col-sm-1 mt-2 p-0 pr-1"> <h3 class="bibliography-year">2023</h3> </div> <div class="col-sm-11 p-0"> <ol class="bibliography"><li> <style>.pub-icon{max-width:105px;height:auto;transition:transform .3s ease;cursor:pointer}.pub-icon:hover{transform:scale(2.5);z-index:10;transform-origin:bottom right;position:relative}</style> <div class="row m-0 mt-3 p-0"> <div class="col-sm-1 p-0 abbr d-flex flex-column align-items-center "> <div class="mb-1 text-center"> <img src="/assets/img/pub/lbp1.png" alt="Patent icon" class="pub-icon"> </div> <span class="badge font-weight-bold light-green darken-1 text-center" style="width: 65px;"> Patent </span> </div> <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-sm-5 pr-sm-2"> <div id="CN110728685B" class="col p-0"> <h5 class="title mb-0">Brain Tissue Segmentation Method Based on Diagonal Voxel Local Binary Pattern Texture Operator</h5> <div class="author"> <nobr><strong>Yongfan Liu</strong>,</nobr> <nobr>Sen Du,</nobr> <nobr><a href="https://cs.seu.edu.cn/kongyouyong/main.htm" target="_blank" rel="external nofollow noopener">Youyong Kong</a>,</nobr> and <nobr><a href="https://baike.baidu.com/item/%E8%88%92%E5%8D%8E%E5%BF%A0/1555089" target="_blank" rel="external nofollow noopener">Huazhong Shu</a>.</nobr> </div> <div> <p class="periodical font-italic"> </p> </div> <div class="col p-0"> <a class="badge orange waves-effect font-weight-light mr-1" data-toggle="collapse" href="#CN110728685B-abstract" role="button" aria-expanded="false" aria-controls="CN110728685B-abstract">Abstract</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://patents.google.com/patent/CN110728685B/en?oq=CN110728685B" target="_blank" rel="external nofollow noopener">HTML</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://sf-liu.github.io//assets/pdf/CN110728685B.pdf" target="_blank">PDF</a> </div> <div class="col mt-2 p-0"> <div id="CN110728685B-abstract" class="collapse"> <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3"> A brain tissue segmentation method based on diagonal voxel local binary pattern texture operator, which performs supervoxel clustering segmentation, extracts diagonal voxel LBP texture features, and uses kNN for feature matching to improve 3D MRI data segmentation efficiency. </div> </div> </div> </div> </div> </div> </li></ol> </div> </div> <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;"> <div class="col-sm-1 mt-2 p-0 pr-1"> <h3 class="bibliography-year">2020</h3> </div> <div class="col-sm-11 p-0"> <ol class="bibliography"><li> <style>.pub-icon{max-width:105px;height:auto;transition:transform .3s ease;cursor:pointer}.pub-icon:hover{transform:scale(2.5);z-index:10;transform-origin:bottom right;position:relative}</style> <div class="row m-0 mt-3 p-0"> <div class="col-sm-1 p-0 abbr d-flex flex-column align-items-center "> <div class="mb-1 text-center"> <img src="/assets/img/pub/lbp.png" alt="IJML icon" class="pub-icon"> </div> <a class="badge font-weight-bold purple darken-1 text-center" style="width: 65px;" href="https://ijml.org/" target="_blank" rel="external nofollow noopener"> IJML </a> </div> <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-sm-5 pr-sm-2"> <div id="liu2020supervoxel" class="col p-0"> <h5 class="title mb-0">Supervoxel Clustering with a Novel 3D Descriptor for Brain Tissue Segmentation</h5> <div class="author"> <nobr><strong>Yongfan Liu</strong>,</nobr> <nobr>Sen Du,</nobr> and <nobr><a href="https://cs.seu.edu.cn/kongyouyong/main.htm" target="_blank" rel="external nofollow noopener">Youyong Kong</a>.</nobr> </div> <div> <p class="periodical font-italic"> In International Journal of Machine Learning and Computing </p> </div> <div class="col p-0"> <a class="badge orange waves-effect font-weight-light mr-1" data-toggle="collapse" href="#liu2020supervoxel-abstract" role="button" aria-expanded="false" aria-controls="liu2020supervoxel-abstract">Abstract</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://www.ijml.org/index.php?m=content&amp;c=index&amp;a=show&amp;catid=107&amp;id=1135" target="_blank" rel="external nofollow noopener">HTML</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://sf-liu.github.io//assets/pdf/964-AM0045.pdf" target="_blank">PDF</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://github.com/sf-Liu/LBP_3D-Operator-Descriptor" target="_blank" rel="external nofollow noopener">Code</a> </div> <div class="col mt-2 p-0"> <div id="liu2020supervoxel-abstract" class="collapse"> <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3"> Accurate segmentation of brain tissues from magnetic resonance imaging (MRI) is of significant importance for clinical application and scientific research. Traditional strategies to handle the 2D images have the limitation of 3D data. In this paper, to overcome these issues, a tissue segmentation approach with supervoxel clustering and the novel 3D texture extraction method are proposed. At first, the simple linear iterative clustering in three-dimension is applied, to reduce the number of calculation objects. Then, a novel local binary pattern in three-dimension is proposed for better discriminate the supervoxels with different tissues. A clustering approach is also developed to classify supervoxels with features into different types of tissues. The labels of supervoxel are finally mapped back to original data to have the tissue type of voxels. The performance of the proposed method is evaluated on the commonly utilized Internet Brain Segmentation Repository 18 dataset. The experiment showed promising results with insufficient trainset. </div> </div> </div> </div> </div> </div> </li></ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Yongfan Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>