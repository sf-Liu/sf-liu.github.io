<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yongfan Liu </title> <meta name="author" content="Yongfan Liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%81&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sf-liu.github.io//"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@700&amp;display=swap" rel="stylesheet"> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%79%6F%6E%67%66%61%6C@%75%63%69.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/sf-Liu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yongfan-liu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0009-0000-9276-5059" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Ln5nT3YAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="/assets/pdf/resume.pdf" title="CV" target="_blank"><i class="ai ai-cv"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications &amp; Patents </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Academic Service </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Yongfan Liu </h1> <p class="desc">PhD Candidate @ University of California, Irvine</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/my_pic-480.webp 480w,/assets/img/my_pic-800.webp 800w,/assets/img/my_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 980px) 285.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/my_pic.jpg?7fe1ab9f0b1bc5859b0ecc4cf2d8009f" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="my_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <div style="text-align: center;"> <p><span style="font-weight: 700;">yongfal [at] uci [dot] edu</span></p> <p>itissteff [at] gmail [dot] com</p> </div> </div> </div> <div class="clearfix"> <h2 id="hi-im-glad-youre-here">Hi, I’m glad you’re here.</h2> <p>I’m currently a Ph.D. candidate in Computer Engineering at the <a href="https://uci.edu/" rel="external nofollow noopener" target="_blank">University of California, Irvine</a>, with a research focus on computer vision and edge computing. My work spans depth estimation, streaming 3D reconstruction, vision–language models (VLMs), speculative decoding for large language models, and efficient model deployment on Qualcomm and NVIDIA platforms.</p> <p>My research aims to extend advanced computer vision capabilities to AR/VR and XR devices. I am a first-author on multiple publications, including work submitted to <strong><em>CVPR</em></strong>, where I applied quantization and hardware-aware optimization techniques to achieve real-time depth estimation on mobile devices. Through these projects, I have developed strong expertise in software–hardware co-design.</p> <p>My recent work focuses on 3D reconstruction for XR scenarios, particularly addressing out-of-memory challenges in long-context video processing through a graph-based memory bank design. In parallel, I am working on air quality estimation using hyperspectral imagery, leveraging vision–language models (VLM) for multi-modal environmental sensing.</p> <p>Previously, I interned at <strong><a href="https://www.nio.com/" rel="external nofollow noopener" target="_blank">NIO</a></strong> (an electric vehicle manufacturer) in San Jose, where I worked on speculative decoding for large language models and deployed them on automotive-grade computing platforms</p> <p><strong><em>I am currently seeking internship opportunities for Spring and Summer 2026.</em></strong></p> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <style>.pub-icon{max-width:105px;height:auto;transition:transform .3s ease;cursor:pointer}.pub-icon:hover{transform:scale(2.5);z-index:10;transform-origin:bottom right;position:relative}</style> <div class="row m-0 mt-3 p-0"> <div class="col-sm-1 p-0 abbr d-flex flex-column align-items-center "> <div class="mb-1 text-center"> <img src="/assets/img/pub/topk_avg.png" alt="arXiv icon" class="pub-icon"> </div> <span class="badge font-weight-bold light-green darken-1 text-center" style="width: 65px;"> arXiv </span> </div> <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-sm-5 pr-sm-2"> <div id="mbvggt" class="col p-0"> <h5 class="title mb-0">MBVGGT: Adapting VGGT for Long Video Sequences via Graph Memory Bank</h5> <div class="author"> <nobr><strong>Yongfan Liu<nobr><em>*</em></nobr></strong>,</nobr> <nobr>Boyuan Tian<nobr><em>*</em></nobr>,</nobr> <nobr><a href="https://www.linkedin.com/in/rahul-singh-309/" target="_blank" rel="external nofollow noopener">Rahul Singh</a>,</nobr> <nobr><a href="https://sadve.cs.illinois.edu/" target="_blank" rel="external nofollow noopener">Sarita Adve</a>,</nobr> and <nobr><a href="https://hyoukjunkwon.com/" target="_blank" rel="external nofollow noopener">Hyoukjun Kwon</a>.</nobr> </div> <div> <p class="periodical font-italic"> In Under Review of CVPR 2026 </p> </div> <div class="col p-0"> <a class="badge orange waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mbvggt-abstract" role="button" aria-expanded="false" aria-controls="mbvggt-abstract">Abstract</a> </div> <div class="col mt-2 p-0"> <div id="mbvggt-abstract" class="collapse"> <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3"> Recent advances in large-scale pretrained visual transformers have achieved remarkable success in delivering high-quality 3D reconstruction results. However, for a stream of inputs (e.g., video), they require heavy global recomputation for each new frame, which also involves quadratic memory costs to the sequence length in attention modules. To address the challenge, techniques such as caching previous keys and values have been explored. However, such approaches still incur quadratic memory costs, which hinder their deployment with long sequence inputs on memory-constrained commodity hardware. To address the challenge, we first make an observation that not all input frames are critical for processing new inputs by a quantitative analysis and utilize that observation to develop a new compute- and memory-efficient vision transformer for 3D reconstruction, MBVGGT. It employs a memory bank that maintains information of important frames only, which significantly reduces memory requirements yet enables high-quality results. Our evaluations demonstrate that MBVGGT achieves comparable accuracy across depth estimation, pose estimation, and 3D reconstruction on long content, while running 6.3x faster than the SOTA model and getting rid of the OOM error. </div> </div> </div> </div> </div> </div> </li> <li> <style>.pub-icon{max-width:105px;height:auto;transition:transform .3s ease;cursor:pointer}.pub-icon:hover{transform:scale(2.5);z-index:10;transform-origin:bottom right;position:relative}</style> <div class="row m-0 mt-3 p-0"> <div class="col-sm-1 p-0 abbr d-flex flex-column align-items-center "> <div class="mb-1 text-center"> <img src="/assets/img/pub/pos_enc.png" alt="CVPR icon" class="pub-icon"> </div> <a class="badge font-weight-bold purple darken-1 text-center" style="width: 65px;" href="https://cvpr.thecvf.com/" target="_blank" rel="external nofollow noopener"> CVPR </a> </div> <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-sm-5 pr-sm-2"> <div id="liu2025efficient" class="col p-0"> <h5 class="title mb-0">Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses</h5> <div class="author"> <nobr><strong>Yongfan Liu</strong>,</nobr> and <nobr><a href="https://hyoukjunkwon.com/" target="_blank" rel="external nofollow noopener">Hyoukjun Kwon</a>.</nobr> </div> <div> <p class="periodical font-italic"> In Proceedings of the Computer Vision and Pattern Recognition Conference 2025. </p> </div> <div class="col p-0"> <a class="badge orange waves-effect font-weight-light mr-1" data-toggle="collapse" href="#liu2025efficient-abstract" role="button" aria-expanded="false" aria-controls="liu2025efficient-abstract">Abstract</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://cvpr.thecvf.com/virtual/2025/poster/33885" target="_blank" rel="external nofollow noopener">HTML</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://sf-liu.github.io//assets/pdf/2411.10013v2.pdf" target="_blank">PDF</a> <a class="badge orange waves-effect font-weight-light mr-1" href="https://github.com/UCI-ISA-Lab/MultiHeadDepth-HomoDepth" target="_blank" rel="external nofollow noopener">Code</a> </div> <div class="col mt-2 p-0"> <div id="liu2025efficient-abstract" class="collapse"> <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3"> Stereo depth estimation is a fundamental component in augmented reality (AR), which requires low latency for real-time processing. However, preprocessing such as rectification and non-ML computations such as cost volume require significant amount of latency exceeding that of an ML model itself, which hinders the real-time processing required by AR. Therefore, we develop alternative approaches to the rectification and cost volume that consider ML acceleration (GPU and NPUs) in recent hardware. For pre-processing, we eliminate it by introducing homography matrix prediction network with a rectification positional encoding (RPE), which delivers both low latency and robustness to unrectified images. For cost volume, we replace it with a group-pointwise convolution-based operator and approximation of cosine similarity based on layernorm and dot product. Based on our approaches, we develop MultiHeadDepth (replacing cost volume) and HomoDepth (MultiHeadDepth + removing pre-processing) models. MultiHeadDepth provides 11.8-30.3% improvements in accuracy and 22.9-25.2% reduction in latency compared to a state-of-the-art depth estimation model for AR glasses from industry. HomoDepth, which can directly process unrectified images, reduces the end-to-end latency by 44.5%. We also introduce a multi-task learning method to handle misaligned stereo inputs on HomoDepth, which reduces the AbsRel error by 10.0-24.3%. The overall results demonstrate the efficacy of our approaches, which not only reduce the inference latency but also improve the model performance. </div> </div> </div> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%6F%6E%67%66%61%6C@%75%63%69.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/sf-Liu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yongfan-liu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0009-0000-9276-5059" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Ln5nT3YAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="/assets/pdf/resume.pdf" title="CV" target="_blank"><i class="ai ai-cv"></i></a> </div> <div class="contact-note"> <p><span style="font-weight: 700;"> The best way to reach me is through my email and LinkedIn DM </span></p> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yongfan Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>